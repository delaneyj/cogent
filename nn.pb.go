// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: nn.proto

/*
	Package cogent is a generated protocol buffer package.

	It is generated from these files:
		nn.proto

	It has these top-level messages:
		Data
		TrainingData
		LayerConfig
		LayerData
		Position
		NeuralNetworkConfiguration
		NeuralNetworkData
		TrainingConfiguration
		MultiSwarmConfiguration
*/
package cogent

import proto "github.com/gogo/protobuf/proto"
import fmt "fmt"
import math "math"

import strconv "strconv"

import strings "strings"
import reflect "reflect"

import binary "encoding/binary"

import io "io"

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.GoGoProtoPackageIsVersion2 // please upgrade the proto package

type Activation int32

const (
	Identity          Activation = 0
	BinaryStep        Activation = 1
	Sigmoid           Activation = 2
	HyperbolicTangent Activation = 3
	ArcTan            Activation = 4
	Softsign          Activation = 5
	ISRU              Activation = 6
	ReLU              Activation = 7
	LeakyReLU         Activation = 8
	ELU               Activation = 9
	SELU              Activation = 10
	SoftPlus          Activation = 11
	BentIdentity      Activation = 12
	Sinusoid          Activation = 13
	Sinc              Activation = 14
	Gaussian          Activation = 15
	Softmax           Activation = 16
	Maxout            Activation = 17
)

var Activation_name = map[int32]string{
	0:  "Identity",
	1:  "BinaryStep",
	2:  "Sigmoid",
	3:  "HyperbolicTangent",
	4:  "ArcTan",
	5:  "Softsign",
	6:  "ISRU",
	7:  "ReLU",
	8:  "LeakyReLU",
	9:  "ELU",
	10: "SELU",
	11: "SoftPlus",
	12: "BentIdentity",
	13: "Sinusoid",
	14: "Sinc",
	15: "Gaussian",
	16: "Softmax",
	17: "Maxout",
}
var Activation_value = map[string]int32{
	"Identity":          0,
	"BinaryStep":        1,
	"Sigmoid":           2,
	"HyperbolicTangent": 3,
	"ArcTan":            4,
	"Softsign":          5,
	"ISRU":              6,
	"ReLU":              7,
	"LeakyReLU":         8,
	"ELU":               9,
	"SELU":              10,
	"SoftPlus":          11,
	"BentIdentity":      12,
	"Sinusoid":          13,
	"Sinc":              14,
	"Gaussian":          15,
	"Softmax":           16,
	"Maxout":            17,
}

func (Activation) EnumDescriptor() ([]byte, []int) { return fileDescriptorNn, []int{0} }

type Loss int32

const (
	Squared                              Loss = 0
	Cross                                Loss = 1
	Exponential                          Loss = 2
	HellingerDistance                    Loss = 3
	KullbackLeiblerDivergence            Loss = 4
	GeneralizedKullbackLeiblerDivergence Loss = 5
	ItakuraSaitoDistance                 Loss = 6
)

var Loss_name = map[int32]string{
	0: "Squared",
	1: "Cross",
	2: "Exponential",
	3: "HellingerDistance",
	4: "KullbackLeiblerDivergence",
	5: "GeneralizedKullbackLeiblerDivergence",
	6: "ItakuraSaitoDistance",
}
var Loss_value = map[string]int32{
	"Squared":                              0,
	"Cross":                                1,
	"Exponential":                          2,
	"HellingerDistance":                    3,
	"KullbackLeiblerDivergence":            4,
	"GeneralizedKullbackLeiblerDivergence": 5,
	"ItakuraSaitoDistance":                 6,
}

func (Loss) EnumDescriptor() ([]byte, []int) { return fileDescriptorNn, []int{1} }

type Data struct {
	Inputs  []float64 `protobuf:"fixed64,1,rep,packed,name=inputs" json:"inputs,omitempty"`
	Outputs []float64 `protobuf:"fixed64,2,rep,packed,name=outputs" json:"outputs,omitempty"`
}

func (m *Data) Reset()                    { *m = Data{} }
func (*Data) ProtoMessage()               {}
func (*Data) Descriptor() ([]byte, []int) { return fileDescriptorNn, []int{0} }

func (m *Data) GetInputs() []float64 {
	if m != nil {
		return m.Inputs
	}
	return nil
}

func (m *Data) GetOutputs() []float64 {
	if m != nil {
		return m.Outputs
	}
	return nil
}

type TrainingData struct {
	Examples []*Data `protobuf:"bytes,1,rep,name=examples" json:"examples,omitempty"`
}

func (m *TrainingData) Reset()                    { *m = TrainingData{} }
func (*TrainingData) ProtoMessage()               {}
func (*TrainingData) Descriptor() ([]byte, []int) { return fileDescriptorNn, []int{1} }

func (m *TrainingData) GetExamples() []*Data {
	if m != nil {
		return m.Examples
	}
	return nil
}

type LayerConfig struct {
	NodeCount  uint32     `protobuf:"varint,1,opt,name=node_count,json=nodeCount,proto3" json:"node_count,omitempty"`
	Activation Activation `protobuf:"varint,2,opt,name=activation,proto3,enum=cogent.Activation" json:"activation,omitempty"`
}

func (m *LayerConfig) Reset()                    { *m = LayerConfig{} }
func (*LayerConfig) ProtoMessage()               {}
func (*LayerConfig) Descriptor() ([]byte, []int) { return fileDescriptorNn, []int{2} }

func (m *LayerConfig) GetNodeCount() uint32 {
	if m != nil {
		return m.NodeCount
	}
	return 0
}

func (m *LayerConfig) GetActivation() Activation {
	if m != nil {
		return m.Activation
	}
	return Identity
}

type LayerData struct {
	NodeCount        uint32     `protobuf:"varint,1,opt,name=node_count,json=nodeCount,proto3" json:"node_count,omitempty"`
	WeightsAndBiases []float64  `protobuf:"fixed64,2,rep,packed,name=weights_and_biases,json=weightsAndBiases" json:"weights_and_biases,omitempty"`
	Velocities       []float64  `protobuf:"fixed64,3,rep,packed,name=velocities" json:"velocities,omitempty"`
	Activation       Activation `protobuf:"varint,4,opt,name=activation,proto3,enum=cogent.Activation" json:"activation,omitempty"`
}

func (m *LayerData) Reset()                    { *m = LayerData{} }
func (*LayerData) ProtoMessage()               {}
func (*LayerData) Descriptor() ([]byte, []int) { return fileDescriptorNn, []int{3} }

func (m *LayerData) GetNodeCount() uint32 {
	if m != nil {
		return m.NodeCount
	}
	return 0
}

func (m *LayerData) GetWeightsAndBiases() []float64 {
	if m != nil {
		return m.WeightsAndBiases
	}
	return nil
}

func (m *LayerData) GetVelocities() []float64 {
	if m != nil {
		return m.Velocities
	}
	return nil
}

func (m *LayerData) GetActivation() Activation {
	if m != nil {
		return m.Activation
	}
	return Identity
}

type Position struct {
	WeightsAndBiases []float64 `protobuf:"fixed64,1,rep,packed,name=weights_and_biases,json=weightsAndBiases" json:"weights_and_biases,omitempty"`
	Loss             float64   `protobuf:"fixed64,2,opt,name=loss,proto3" json:"loss,omitempty"`
}

func (m *Position) Reset()                    { *m = Position{} }
func (*Position) ProtoMessage()               {}
func (*Position) Descriptor() ([]byte, []int) { return fileDescriptorNn, []int{4} }

func (m *Position) GetWeightsAndBiases() []float64 {
	if m != nil {
		return m.WeightsAndBiases
	}
	return nil
}

func (m *Position) GetLoss() float64 {
	if m != nil {
		return m.Loss
	}
	return 0
}

type NeuralNetworkConfiguration struct {
	Loss         Loss           `protobuf:"varint,1,opt,name=loss,proto3,enum=cogent.Loss" json:"loss,omitempty"`
	InputCount   uint32         `protobuf:"varint,2,opt,name=input_count,json=inputCount,proto3" json:"input_count,omitempty"`
	LayerConfigs []*LayerConfig `protobuf:"bytes,3,rep,name=layer_configs,json=layerConfigs" json:"layer_configs,omitempty"`
}

func (m *NeuralNetworkConfiguration) Reset()                    { *m = NeuralNetworkConfiguration{} }
func (*NeuralNetworkConfiguration) ProtoMessage()               {}
func (*NeuralNetworkConfiguration) Descriptor() ([]byte, []int) { return fileDescriptorNn, []int{5} }

func (m *NeuralNetworkConfiguration) GetLoss() Loss {
	if m != nil {
		return m.Loss
	}
	return Squared
}

func (m *NeuralNetworkConfiguration) GetInputCount() uint32 {
	if m != nil {
		return m.InputCount
	}
	return 0
}

func (m *NeuralNetworkConfiguration) GetLayerConfigs() []*LayerConfig {
	if m != nil {
		return m.LayerConfigs
	}
	return nil
}

type NeuralNetworkData struct {
	Loss        Loss         `protobuf:"varint,1,opt,name=loss,proto3,enum=cogent.Loss" json:"loss,omitempty"`
	Layers      []*LayerData `protobuf:"bytes,2,rep,name=layers" json:"layers,omitempty"`
	CurrentLoss float64      `protobuf:"fixed64,3,opt,name=current_loss,json=currentLoss,proto3" json:"current_loss,omitempty"`
	Best        *Position    `protobuf:"bytes,4,opt,name=best" json:"best,omitempty"`
}

func (m *NeuralNetworkData) Reset()                    { *m = NeuralNetworkData{} }
func (*NeuralNetworkData) ProtoMessage()               {}
func (*NeuralNetworkData) Descriptor() ([]byte, []int) { return fileDescriptorNn, []int{6} }

func (m *NeuralNetworkData) GetLoss() Loss {
	if m != nil {
		return m.Loss
	}
	return Squared
}

func (m *NeuralNetworkData) GetLayers() []*LayerData {
	if m != nil {
		return m.Layers
	}
	return nil
}

func (m *NeuralNetworkData) GetCurrentLoss() float64 {
	if m != nil {
		return m.CurrentLoss
	}
	return 0
}

func (m *NeuralNetworkData) GetBest() *Position {
	if m != nil {
		return m.Best
	}
	return nil
}

type TrainingConfiguration struct {
	InertiaWeight     float64 `protobuf:"fixed64,1,opt,name=inertia_weight,json=inertiaWeight,proto3" json:"inertia_weight,omitempty"`
	CognitiveWeight   float64 `protobuf:"fixed64,2,opt,name=cognitive_weight,json=cognitiveWeight,proto3" json:"cognitive_weight,omitempty"`
	SocialWeight      float64 `protobuf:"fixed64,3,opt,name=social_weight,json=socialWeight,proto3" json:"social_weight,omitempty"`
	GlobalWeight      float64 `protobuf:"fixed64,4,opt,name=global_weight,json=globalWeight,proto3" json:"global_weight,omitempty"`
	MaxIterations     uint32  `protobuf:"varint,5,opt,name=maxIterations,proto3" json:"maxIterations,omitempty"`
	TargetAccuracy    float64 `protobuf:"fixed64,6,opt,name=target_accuracy,json=targetAccuracy,proto3" json:"target_accuracy,omitempty"`
	WeightRange       float64 `protobuf:"fixed64,7,opt,name=weight_range,json=weightRange,proto3" json:"weight_range,omitempty"`
	WeightDecayRate   float64 `protobuf:"fixed64,8,opt,name=weight_decay_rate,json=weightDecayRate,proto3" json:"weight_decay_rate,omitempty"`
	ProbablityOfDeath float64 `protobuf:"fixed64,9,opt,name=probablity_of_death,json=probablityOfDeath,proto3" json:"probablity_of_death,omitempty"`
}

func (m *TrainingConfiguration) Reset()                    { *m = TrainingConfiguration{} }
func (*TrainingConfiguration) ProtoMessage()               {}
func (*TrainingConfiguration) Descriptor() ([]byte, []int) { return fileDescriptorNn, []int{7} }

func (m *TrainingConfiguration) GetInertiaWeight() float64 {
	if m != nil {
		return m.InertiaWeight
	}
	return 0
}

func (m *TrainingConfiguration) GetCognitiveWeight() float64 {
	if m != nil {
		return m.CognitiveWeight
	}
	return 0
}

func (m *TrainingConfiguration) GetSocialWeight() float64 {
	if m != nil {
		return m.SocialWeight
	}
	return 0
}

func (m *TrainingConfiguration) GetGlobalWeight() float64 {
	if m != nil {
		return m.GlobalWeight
	}
	return 0
}

func (m *TrainingConfiguration) GetMaxIterations() uint32 {
	if m != nil {
		return m.MaxIterations
	}
	return 0
}

func (m *TrainingConfiguration) GetTargetAccuracy() float64 {
	if m != nil {
		return m.TargetAccuracy
	}
	return 0
}

func (m *TrainingConfiguration) GetWeightRange() float64 {
	if m != nil {
		return m.WeightRange
	}
	return 0
}

func (m *TrainingConfiguration) GetWeightDecayRate() float64 {
	if m != nil {
		return m.WeightDecayRate
	}
	return 0
}

func (m *TrainingConfiguration) GetProbablityOfDeath() float64 {
	if m != nil {
		return m.ProbablityOfDeath
	}
	return 0
}

type MultiSwarmConfiguration struct {
	NeuralNetworkConfiguration *NeuralNetworkConfiguration `protobuf:"bytes,2,opt,name=neural_network_configuration,json=neuralNetworkConfiguration" json:"neural_network_configuration,omitempty"`
	SwarmCount                 uint32                      `protobuf:"varint,3,opt,name=swarm_count,json=swarmCount,proto3" json:"swarm_count,omitempty"`
	ParticleCount              uint32                      `protobuf:"varint,4,opt,name=particle_count,json=particleCount,proto3" json:"particle_count,omitempty"`
}

func (m *MultiSwarmConfiguration) Reset()                    { *m = MultiSwarmConfiguration{} }
func (*MultiSwarmConfiguration) ProtoMessage()               {}
func (*MultiSwarmConfiguration) Descriptor() ([]byte, []int) { return fileDescriptorNn, []int{8} }

func (m *MultiSwarmConfiguration) GetNeuralNetworkConfiguration() *NeuralNetworkConfiguration {
	if m != nil {
		return m.NeuralNetworkConfiguration
	}
	return nil
}

func (m *MultiSwarmConfiguration) GetSwarmCount() uint32 {
	if m != nil {
		return m.SwarmCount
	}
	return 0
}

func (m *MultiSwarmConfiguration) GetParticleCount() uint32 {
	if m != nil {
		return m.ParticleCount
	}
	return 0
}

func init() {
	proto.RegisterType((*Data)(nil), "cogent.Data")
	proto.RegisterType((*TrainingData)(nil), "cogent.TrainingData")
	proto.RegisterType((*LayerConfig)(nil), "cogent.LayerConfig")
	proto.RegisterType((*LayerData)(nil), "cogent.LayerData")
	proto.RegisterType((*Position)(nil), "cogent.Position")
	proto.RegisterType((*NeuralNetworkConfiguration)(nil), "cogent.NeuralNetworkConfiguration")
	proto.RegisterType((*NeuralNetworkData)(nil), "cogent.NeuralNetworkData")
	proto.RegisterType((*TrainingConfiguration)(nil), "cogent.TrainingConfiguration")
	proto.RegisterType((*MultiSwarmConfiguration)(nil), "cogent.MultiSwarmConfiguration")
	proto.RegisterEnum("cogent.Activation", Activation_name, Activation_value)
	proto.RegisterEnum("cogent.Loss", Loss_name, Loss_value)
}
func (x Activation) String() string {
	s, ok := Activation_name[int32(x)]
	if ok {
		return s
	}
	return strconv.Itoa(int(x))
}
func (x Loss) String() string {
	s, ok := Loss_name[int32(x)]
	if ok {
		return s
	}
	return strconv.Itoa(int(x))
}
func (this *Data) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*Data)
	if !ok {
		that2, ok := that.(Data)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if len(this.Inputs) != len(that1.Inputs) {
		return false
	}
	for i := range this.Inputs {
		if this.Inputs[i] != that1.Inputs[i] {
			return false
		}
	}
	if len(this.Outputs) != len(that1.Outputs) {
		return false
	}
	for i := range this.Outputs {
		if this.Outputs[i] != that1.Outputs[i] {
			return false
		}
	}
	return true
}
func (this *TrainingData) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*TrainingData)
	if !ok {
		that2, ok := that.(TrainingData)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if len(this.Examples) != len(that1.Examples) {
		return false
	}
	for i := range this.Examples {
		if !this.Examples[i].Equal(that1.Examples[i]) {
			return false
		}
	}
	return true
}
func (this *LayerConfig) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*LayerConfig)
	if !ok {
		that2, ok := that.(LayerConfig)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.NodeCount != that1.NodeCount {
		return false
	}
	if this.Activation != that1.Activation {
		return false
	}
	return true
}
func (this *LayerData) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*LayerData)
	if !ok {
		that2, ok := that.(LayerData)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.NodeCount != that1.NodeCount {
		return false
	}
	if len(this.WeightsAndBiases) != len(that1.WeightsAndBiases) {
		return false
	}
	for i := range this.WeightsAndBiases {
		if this.WeightsAndBiases[i] != that1.WeightsAndBiases[i] {
			return false
		}
	}
	if len(this.Velocities) != len(that1.Velocities) {
		return false
	}
	for i := range this.Velocities {
		if this.Velocities[i] != that1.Velocities[i] {
			return false
		}
	}
	if this.Activation != that1.Activation {
		return false
	}
	return true
}
func (this *Position) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*Position)
	if !ok {
		that2, ok := that.(Position)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if len(this.WeightsAndBiases) != len(that1.WeightsAndBiases) {
		return false
	}
	for i := range this.WeightsAndBiases {
		if this.WeightsAndBiases[i] != that1.WeightsAndBiases[i] {
			return false
		}
	}
	if this.Loss != that1.Loss {
		return false
	}
	return true
}
func (this *NeuralNetworkConfiguration) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*NeuralNetworkConfiguration)
	if !ok {
		that2, ok := that.(NeuralNetworkConfiguration)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.Loss != that1.Loss {
		return false
	}
	if this.InputCount != that1.InputCount {
		return false
	}
	if len(this.LayerConfigs) != len(that1.LayerConfigs) {
		return false
	}
	for i := range this.LayerConfigs {
		if !this.LayerConfigs[i].Equal(that1.LayerConfigs[i]) {
			return false
		}
	}
	return true
}
func (this *NeuralNetworkData) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*NeuralNetworkData)
	if !ok {
		that2, ok := that.(NeuralNetworkData)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.Loss != that1.Loss {
		return false
	}
	if len(this.Layers) != len(that1.Layers) {
		return false
	}
	for i := range this.Layers {
		if !this.Layers[i].Equal(that1.Layers[i]) {
			return false
		}
	}
	if this.CurrentLoss != that1.CurrentLoss {
		return false
	}
	if !this.Best.Equal(that1.Best) {
		return false
	}
	return true
}
func (this *TrainingConfiguration) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*TrainingConfiguration)
	if !ok {
		that2, ok := that.(TrainingConfiguration)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if this.InertiaWeight != that1.InertiaWeight {
		return false
	}
	if this.CognitiveWeight != that1.CognitiveWeight {
		return false
	}
	if this.SocialWeight != that1.SocialWeight {
		return false
	}
	if this.GlobalWeight != that1.GlobalWeight {
		return false
	}
	if this.MaxIterations != that1.MaxIterations {
		return false
	}
	if this.TargetAccuracy != that1.TargetAccuracy {
		return false
	}
	if this.WeightRange != that1.WeightRange {
		return false
	}
	if this.WeightDecayRate != that1.WeightDecayRate {
		return false
	}
	if this.ProbablityOfDeath != that1.ProbablityOfDeath {
		return false
	}
	return true
}
func (this *MultiSwarmConfiguration) Equal(that interface{}) bool {
	if that == nil {
		return this == nil
	}

	that1, ok := that.(*MultiSwarmConfiguration)
	if !ok {
		that2, ok := that.(MultiSwarmConfiguration)
		if ok {
			that1 = &that2
		} else {
			return false
		}
	}
	if that1 == nil {
		return this == nil
	} else if this == nil {
		return false
	}
	if !this.NeuralNetworkConfiguration.Equal(that1.NeuralNetworkConfiguration) {
		return false
	}
	if this.SwarmCount != that1.SwarmCount {
		return false
	}
	if this.ParticleCount != that1.ParticleCount {
		return false
	}
	return true
}
func (this *Data) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 6)
	s = append(s, "&cogent.Data{")
	s = append(s, "Inputs: "+fmt.Sprintf("%#v", this.Inputs)+",\n")
	s = append(s, "Outputs: "+fmt.Sprintf("%#v", this.Outputs)+",\n")
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *TrainingData) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 5)
	s = append(s, "&cogent.TrainingData{")
	if this.Examples != nil {
		s = append(s, "Examples: "+fmt.Sprintf("%#v", this.Examples)+",\n")
	}
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *LayerConfig) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 6)
	s = append(s, "&cogent.LayerConfig{")
	s = append(s, "NodeCount: "+fmt.Sprintf("%#v", this.NodeCount)+",\n")
	s = append(s, "Activation: "+fmt.Sprintf("%#v", this.Activation)+",\n")
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *LayerData) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 8)
	s = append(s, "&cogent.LayerData{")
	s = append(s, "NodeCount: "+fmt.Sprintf("%#v", this.NodeCount)+",\n")
	s = append(s, "WeightsAndBiases: "+fmt.Sprintf("%#v", this.WeightsAndBiases)+",\n")
	s = append(s, "Velocities: "+fmt.Sprintf("%#v", this.Velocities)+",\n")
	s = append(s, "Activation: "+fmt.Sprintf("%#v", this.Activation)+",\n")
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *Position) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 6)
	s = append(s, "&cogent.Position{")
	s = append(s, "WeightsAndBiases: "+fmt.Sprintf("%#v", this.WeightsAndBiases)+",\n")
	s = append(s, "Loss: "+fmt.Sprintf("%#v", this.Loss)+",\n")
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *NeuralNetworkConfiguration) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 7)
	s = append(s, "&cogent.NeuralNetworkConfiguration{")
	s = append(s, "Loss: "+fmt.Sprintf("%#v", this.Loss)+",\n")
	s = append(s, "InputCount: "+fmt.Sprintf("%#v", this.InputCount)+",\n")
	if this.LayerConfigs != nil {
		s = append(s, "LayerConfigs: "+fmt.Sprintf("%#v", this.LayerConfigs)+",\n")
	}
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *NeuralNetworkData) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 8)
	s = append(s, "&cogent.NeuralNetworkData{")
	s = append(s, "Loss: "+fmt.Sprintf("%#v", this.Loss)+",\n")
	if this.Layers != nil {
		s = append(s, "Layers: "+fmt.Sprintf("%#v", this.Layers)+",\n")
	}
	s = append(s, "CurrentLoss: "+fmt.Sprintf("%#v", this.CurrentLoss)+",\n")
	if this.Best != nil {
		s = append(s, "Best: "+fmt.Sprintf("%#v", this.Best)+",\n")
	}
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *TrainingConfiguration) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 13)
	s = append(s, "&cogent.TrainingConfiguration{")
	s = append(s, "InertiaWeight: "+fmt.Sprintf("%#v", this.InertiaWeight)+",\n")
	s = append(s, "CognitiveWeight: "+fmt.Sprintf("%#v", this.CognitiveWeight)+",\n")
	s = append(s, "SocialWeight: "+fmt.Sprintf("%#v", this.SocialWeight)+",\n")
	s = append(s, "GlobalWeight: "+fmt.Sprintf("%#v", this.GlobalWeight)+",\n")
	s = append(s, "MaxIterations: "+fmt.Sprintf("%#v", this.MaxIterations)+",\n")
	s = append(s, "TargetAccuracy: "+fmt.Sprintf("%#v", this.TargetAccuracy)+",\n")
	s = append(s, "WeightRange: "+fmt.Sprintf("%#v", this.WeightRange)+",\n")
	s = append(s, "WeightDecayRate: "+fmt.Sprintf("%#v", this.WeightDecayRate)+",\n")
	s = append(s, "ProbablityOfDeath: "+fmt.Sprintf("%#v", this.ProbablityOfDeath)+",\n")
	s = append(s, "}")
	return strings.Join(s, "")
}
func (this *MultiSwarmConfiguration) GoString() string {
	if this == nil {
		return "nil"
	}
	s := make([]string, 0, 7)
	s = append(s, "&cogent.MultiSwarmConfiguration{")
	if this.NeuralNetworkConfiguration != nil {
		s = append(s, "NeuralNetworkConfiguration: "+fmt.Sprintf("%#v", this.NeuralNetworkConfiguration)+",\n")
	}
	s = append(s, "SwarmCount: "+fmt.Sprintf("%#v", this.SwarmCount)+",\n")
	s = append(s, "ParticleCount: "+fmt.Sprintf("%#v", this.ParticleCount)+",\n")
	s = append(s, "}")
	return strings.Join(s, "")
}
func valueToGoStringNn(v interface{}, typ string) string {
	rv := reflect.ValueOf(v)
	if rv.IsNil() {
		return "nil"
	}
	pv := reflect.Indirect(rv).Interface()
	return fmt.Sprintf("func(v %v) *%v { return &v } ( %#v )", typ, typ, pv)
}
func (m *Data) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *Data) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if len(m.Inputs) > 0 {
		dAtA[i] = 0xa
		i++
		i = encodeVarintNn(dAtA, i, uint64(len(m.Inputs)*8))
		for _, num := range m.Inputs {
			f1 := math.Float64bits(float64(num))
			binary.LittleEndian.PutUint64(dAtA[i:], uint64(f1))
			i += 8
		}
	}
	if len(m.Outputs) > 0 {
		dAtA[i] = 0x12
		i++
		i = encodeVarintNn(dAtA, i, uint64(len(m.Outputs)*8))
		for _, num := range m.Outputs {
			f2 := math.Float64bits(float64(num))
			binary.LittleEndian.PutUint64(dAtA[i:], uint64(f2))
			i += 8
		}
	}
	return i, nil
}

func (m *TrainingData) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *TrainingData) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if len(m.Examples) > 0 {
		for _, msg := range m.Examples {
			dAtA[i] = 0xa
			i++
			i = encodeVarintNn(dAtA, i, uint64(msg.Size()))
			n, err := msg.MarshalTo(dAtA[i:])
			if err != nil {
				return 0, err
			}
			i += n
		}
	}
	return i, nil
}

func (m *LayerConfig) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *LayerConfig) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.NodeCount != 0 {
		dAtA[i] = 0x8
		i++
		i = encodeVarintNn(dAtA, i, uint64(m.NodeCount))
	}
	if m.Activation != 0 {
		dAtA[i] = 0x10
		i++
		i = encodeVarintNn(dAtA, i, uint64(m.Activation))
	}
	return i, nil
}

func (m *LayerData) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *LayerData) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.NodeCount != 0 {
		dAtA[i] = 0x8
		i++
		i = encodeVarintNn(dAtA, i, uint64(m.NodeCount))
	}
	if len(m.WeightsAndBiases) > 0 {
		dAtA[i] = 0x12
		i++
		i = encodeVarintNn(dAtA, i, uint64(len(m.WeightsAndBiases)*8))
		for _, num := range m.WeightsAndBiases {
			f3 := math.Float64bits(float64(num))
			binary.LittleEndian.PutUint64(dAtA[i:], uint64(f3))
			i += 8
		}
	}
	if len(m.Velocities) > 0 {
		dAtA[i] = 0x1a
		i++
		i = encodeVarintNn(dAtA, i, uint64(len(m.Velocities)*8))
		for _, num := range m.Velocities {
			f4 := math.Float64bits(float64(num))
			binary.LittleEndian.PutUint64(dAtA[i:], uint64(f4))
			i += 8
		}
	}
	if m.Activation != 0 {
		dAtA[i] = 0x20
		i++
		i = encodeVarintNn(dAtA, i, uint64(m.Activation))
	}
	return i, nil
}

func (m *Position) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *Position) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if len(m.WeightsAndBiases) > 0 {
		dAtA[i] = 0xa
		i++
		i = encodeVarintNn(dAtA, i, uint64(len(m.WeightsAndBiases)*8))
		for _, num := range m.WeightsAndBiases {
			f5 := math.Float64bits(float64(num))
			binary.LittleEndian.PutUint64(dAtA[i:], uint64(f5))
			i += 8
		}
	}
	if m.Loss != 0 {
		dAtA[i] = 0x11
		i++
		binary.LittleEndian.PutUint64(dAtA[i:], uint64(math.Float64bits(float64(m.Loss))))
		i += 8
	}
	return i, nil
}

func (m *NeuralNetworkConfiguration) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *NeuralNetworkConfiguration) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.Loss != 0 {
		dAtA[i] = 0x8
		i++
		i = encodeVarintNn(dAtA, i, uint64(m.Loss))
	}
	if m.InputCount != 0 {
		dAtA[i] = 0x10
		i++
		i = encodeVarintNn(dAtA, i, uint64(m.InputCount))
	}
	if len(m.LayerConfigs) > 0 {
		for _, msg := range m.LayerConfigs {
			dAtA[i] = 0x1a
			i++
			i = encodeVarintNn(dAtA, i, uint64(msg.Size()))
			n, err := msg.MarshalTo(dAtA[i:])
			if err != nil {
				return 0, err
			}
			i += n
		}
	}
	return i, nil
}

func (m *NeuralNetworkData) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *NeuralNetworkData) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.Loss != 0 {
		dAtA[i] = 0x8
		i++
		i = encodeVarintNn(dAtA, i, uint64(m.Loss))
	}
	if len(m.Layers) > 0 {
		for _, msg := range m.Layers {
			dAtA[i] = 0x12
			i++
			i = encodeVarintNn(dAtA, i, uint64(msg.Size()))
			n, err := msg.MarshalTo(dAtA[i:])
			if err != nil {
				return 0, err
			}
			i += n
		}
	}
	if m.CurrentLoss != 0 {
		dAtA[i] = 0x19
		i++
		binary.LittleEndian.PutUint64(dAtA[i:], uint64(math.Float64bits(float64(m.CurrentLoss))))
		i += 8
	}
	if m.Best != nil {
		dAtA[i] = 0x22
		i++
		i = encodeVarintNn(dAtA, i, uint64(m.Best.Size()))
		n6, err := m.Best.MarshalTo(dAtA[i:])
		if err != nil {
			return 0, err
		}
		i += n6
	}
	return i, nil
}

func (m *TrainingConfiguration) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *TrainingConfiguration) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.InertiaWeight != 0 {
		dAtA[i] = 0x9
		i++
		binary.LittleEndian.PutUint64(dAtA[i:], uint64(math.Float64bits(float64(m.InertiaWeight))))
		i += 8
	}
	if m.CognitiveWeight != 0 {
		dAtA[i] = 0x11
		i++
		binary.LittleEndian.PutUint64(dAtA[i:], uint64(math.Float64bits(float64(m.CognitiveWeight))))
		i += 8
	}
	if m.SocialWeight != 0 {
		dAtA[i] = 0x19
		i++
		binary.LittleEndian.PutUint64(dAtA[i:], uint64(math.Float64bits(float64(m.SocialWeight))))
		i += 8
	}
	if m.GlobalWeight != 0 {
		dAtA[i] = 0x21
		i++
		binary.LittleEndian.PutUint64(dAtA[i:], uint64(math.Float64bits(float64(m.GlobalWeight))))
		i += 8
	}
	if m.MaxIterations != 0 {
		dAtA[i] = 0x28
		i++
		i = encodeVarintNn(dAtA, i, uint64(m.MaxIterations))
	}
	if m.TargetAccuracy != 0 {
		dAtA[i] = 0x31
		i++
		binary.LittleEndian.PutUint64(dAtA[i:], uint64(math.Float64bits(float64(m.TargetAccuracy))))
		i += 8
	}
	if m.WeightRange != 0 {
		dAtA[i] = 0x39
		i++
		binary.LittleEndian.PutUint64(dAtA[i:], uint64(math.Float64bits(float64(m.WeightRange))))
		i += 8
	}
	if m.WeightDecayRate != 0 {
		dAtA[i] = 0x41
		i++
		binary.LittleEndian.PutUint64(dAtA[i:], uint64(math.Float64bits(float64(m.WeightDecayRate))))
		i += 8
	}
	if m.ProbablityOfDeath != 0 {
		dAtA[i] = 0x49
		i++
		binary.LittleEndian.PutUint64(dAtA[i:], uint64(math.Float64bits(float64(m.ProbablityOfDeath))))
		i += 8
	}
	return i, nil
}

func (m *MultiSwarmConfiguration) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalTo(dAtA)
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *MultiSwarmConfiguration) MarshalTo(dAtA []byte) (int, error) {
	var i int
	_ = i
	var l int
	_ = l
	if m.NeuralNetworkConfiguration != nil {
		dAtA[i] = 0x12
		i++
		i = encodeVarintNn(dAtA, i, uint64(m.NeuralNetworkConfiguration.Size()))
		n7, err := m.NeuralNetworkConfiguration.MarshalTo(dAtA[i:])
		if err != nil {
			return 0, err
		}
		i += n7
	}
	if m.SwarmCount != 0 {
		dAtA[i] = 0x18
		i++
		i = encodeVarintNn(dAtA, i, uint64(m.SwarmCount))
	}
	if m.ParticleCount != 0 {
		dAtA[i] = 0x20
		i++
		i = encodeVarintNn(dAtA, i, uint64(m.ParticleCount))
	}
	return i, nil
}

func encodeVarintNn(dAtA []byte, offset int, v uint64) int {
	for v >= 1<<7 {
		dAtA[offset] = uint8(v&0x7f | 0x80)
		v >>= 7
		offset++
	}
	dAtA[offset] = uint8(v)
	return offset + 1
}
func (m *Data) Size() (n int) {
	var l int
	_ = l
	if len(m.Inputs) > 0 {
		n += 1 + sovNn(uint64(len(m.Inputs)*8)) + len(m.Inputs)*8
	}
	if len(m.Outputs) > 0 {
		n += 1 + sovNn(uint64(len(m.Outputs)*8)) + len(m.Outputs)*8
	}
	return n
}

func (m *TrainingData) Size() (n int) {
	var l int
	_ = l
	if len(m.Examples) > 0 {
		for _, e := range m.Examples {
			l = e.Size()
			n += 1 + l + sovNn(uint64(l))
		}
	}
	return n
}

func (m *LayerConfig) Size() (n int) {
	var l int
	_ = l
	if m.NodeCount != 0 {
		n += 1 + sovNn(uint64(m.NodeCount))
	}
	if m.Activation != 0 {
		n += 1 + sovNn(uint64(m.Activation))
	}
	return n
}

func (m *LayerData) Size() (n int) {
	var l int
	_ = l
	if m.NodeCount != 0 {
		n += 1 + sovNn(uint64(m.NodeCount))
	}
	if len(m.WeightsAndBiases) > 0 {
		n += 1 + sovNn(uint64(len(m.WeightsAndBiases)*8)) + len(m.WeightsAndBiases)*8
	}
	if len(m.Velocities) > 0 {
		n += 1 + sovNn(uint64(len(m.Velocities)*8)) + len(m.Velocities)*8
	}
	if m.Activation != 0 {
		n += 1 + sovNn(uint64(m.Activation))
	}
	return n
}

func (m *Position) Size() (n int) {
	var l int
	_ = l
	if len(m.WeightsAndBiases) > 0 {
		n += 1 + sovNn(uint64(len(m.WeightsAndBiases)*8)) + len(m.WeightsAndBiases)*8
	}
	if m.Loss != 0 {
		n += 9
	}
	return n
}

func (m *NeuralNetworkConfiguration) Size() (n int) {
	var l int
	_ = l
	if m.Loss != 0 {
		n += 1 + sovNn(uint64(m.Loss))
	}
	if m.InputCount != 0 {
		n += 1 + sovNn(uint64(m.InputCount))
	}
	if len(m.LayerConfigs) > 0 {
		for _, e := range m.LayerConfigs {
			l = e.Size()
			n += 1 + l + sovNn(uint64(l))
		}
	}
	return n
}

func (m *NeuralNetworkData) Size() (n int) {
	var l int
	_ = l
	if m.Loss != 0 {
		n += 1 + sovNn(uint64(m.Loss))
	}
	if len(m.Layers) > 0 {
		for _, e := range m.Layers {
			l = e.Size()
			n += 1 + l + sovNn(uint64(l))
		}
	}
	if m.CurrentLoss != 0 {
		n += 9
	}
	if m.Best != nil {
		l = m.Best.Size()
		n += 1 + l + sovNn(uint64(l))
	}
	return n
}

func (m *TrainingConfiguration) Size() (n int) {
	var l int
	_ = l
	if m.InertiaWeight != 0 {
		n += 9
	}
	if m.CognitiveWeight != 0 {
		n += 9
	}
	if m.SocialWeight != 0 {
		n += 9
	}
	if m.GlobalWeight != 0 {
		n += 9
	}
	if m.MaxIterations != 0 {
		n += 1 + sovNn(uint64(m.MaxIterations))
	}
	if m.TargetAccuracy != 0 {
		n += 9
	}
	if m.WeightRange != 0 {
		n += 9
	}
	if m.WeightDecayRate != 0 {
		n += 9
	}
	if m.ProbablityOfDeath != 0 {
		n += 9
	}
	return n
}

func (m *MultiSwarmConfiguration) Size() (n int) {
	var l int
	_ = l
	if m.NeuralNetworkConfiguration != nil {
		l = m.NeuralNetworkConfiguration.Size()
		n += 1 + l + sovNn(uint64(l))
	}
	if m.SwarmCount != 0 {
		n += 1 + sovNn(uint64(m.SwarmCount))
	}
	if m.ParticleCount != 0 {
		n += 1 + sovNn(uint64(m.ParticleCount))
	}
	return n
}

func sovNn(x uint64) (n int) {
	for {
		n++
		x >>= 7
		if x == 0 {
			break
		}
	}
	return n
}
func sozNn(x uint64) (n int) {
	return sovNn(uint64((x << 1) ^ uint64((int64(x) >> 63))))
}
func (this *Data) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&Data{`,
		`Inputs:` + fmt.Sprintf("%v", this.Inputs) + `,`,
		`Outputs:` + fmt.Sprintf("%v", this.Outputs) + `,`,
		`}`,
	}, "")
	return s
}
func (this *TrainingData) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&TrainingData{`,
		`Examples:` + strings.Replace(fmt.Sprintf("%v", this.Examples), "Data", "Data", 1) + `,`,
		`}`,
	}, "")
	return s
}
func (this *LayerConfig) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&LayerConfig{`,
		`NodeCount:` + fmt.Sprintf("%v", this.NodeCount) + `,`,
		`Activation:` + fmt.Sprintf("%v", this.Activation) + `,`,
		`}`,
	}, "")
	return s
}
func (this *LayerData) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&LayerData{`,
		`NodeCount:` + fmt.Sprintf("%v", this.NodeCount) + `,`,
		`WeightsAndBiases:` + fmt.Sprintf("%v", this.WeightsAndBiases) + `,`,
		`Velocities:` + fmt.Sprintf("%v", this.Velocities) + `,`,
		`Activation:` + fmt.Sprintf("%v", this.Activation) + `,`,
		`}`,
	}, "")
	return s
}
func (this *Position) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&Position{`,
		`WeightsAndBiases:` + fmt.Sprintf("%v", this.WeightsAndBiases) + `,`,
		`Loss:` + fmt.Sprintf("%v", this.Loss) + `,`,
		`}`,
	}, "")
	return s
}
func (this *NeuralNetworkConfiguration) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&NeuralNetworkConfiguration{`,
		`Loss:` + fmt.Sprintf("%v", this.Loss) + `,`,
		`InputCount:` + fmt.Sprintf("%v", this.InputCount) + `,`,
		`LayerConfigs:` + strings.Replace(fmt.Sprintf("%v", this.LayerConfigs), "LayerConfig", "LayerConfig", 1) + `,`,
		`}`,
	}, "")
	return s
}
func (this *NeuralNetworkData) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&NeuralNetworkData{`,
		`Loss:` + fmt.Sprintf("%v", this.Loss) + `,`,
		`Layers:` + strings.Replace(fmt.Sprintf("%v", this.Layers), "LayerData", "LayerData", 1) + `,`,
		`CurrentLoss:` + fmt.Sprintf("%v", this.CurrentLoss) + `,`,
		`Best:` + strings.Replace(fmt.Sprintf("%v", this.Best), "Position", "Position", 1) + `,`,
		`}`,
	}, "")
	return s
}
func (this *TrainingConfiguration) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&TrainingConfiguration{`,
		`InertiaWeight:` + fmt.Sprintf("%v", this.InertiaWeight) + `,`,
		`CognitiveWeight:` + fmt.Sprintf("%v", this.CognitiveWeight) + `,`,
		`SocialWeight:` + fmt.Sprintf("%v", this.SocialWeight) + `,`,
		`GlobalWeight:` + fmt.Sprintf("%v", this.GlobalWeight) + `,`,
		`MaxIterations:` + fmt.Sprintf("%v", this.MaxIterations) + `,`,
		`TargetAccuracy:` + fmt.Sprintf("%v", this.TargetAccuracy) + `,`,
		`WeightRange:` + fmt.Sprintf("%v", this.WeightRange) + `,`,
		`WeightDecayRate:` + fmt.Sprintf("%v", this.WeightDecayRate) + `,`,
		`ProbablityOfDeath:` + fmt.Sprintf("%v", this.ProbablityOfDeath) + `,`,
		`}`,
	}, "")
	return s
}
func (this *MultiSwarmConfiguration) String() string {
	if this == nil {
		return "nil"
	}
	s := strings.Join([]string{`&MultiSwarmConfiguration{`,
		`NeuralNetworkConfiguration:` + strings.Replace(fmt.Sprintf("%v", this.NeuralNetworkConfiguration), "NeuralNetworkConfiguration", "NeuralNetworkConfiguration", 1) + `,`,
		`SwarmCount:` + fmt.Sprintf("%v", this.SwarmCount) + `,`,
		`ParticleCount:` + fmt.Sprintf("%v", this.ParticleCount) + `,`,
		`}`,
	}, "")
	return s
}
func valueToStringNn(v interface{}) string {
	rv := reflect.ValueOf(v)
	if rv.IsNil() {
		return "nil"
	}
	pv := reflect.Indirect(rv).Interface()
	return fmt.Sprintf("*%v", pv)
}
func (m *Data) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowNn
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: Data: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: Data: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType == 1 {
				var v uint64
				if (iNdEx + 8) > l {
					return io.ErrUnexpectedEOF
				}
				v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
				iNdEx += 8
				v2 := float64(math.Float64frombits(v))
				m.Inputs = append(m.Inputs, v2)
			} else if wireType == 2 {
				var packedLen int
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowNn
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					packedLen |= (int(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				if packedLen < 0 {
					return ErrInvalidLengthNn
				}
				postIndex := iNdEx + packedLen
				if postIndex > l {
					return io.ErrUnexpectedEOF
				}
				for iNdEx < postIndex {
					var v uint64
					if (iNdEx + 8) > l {
						return io.ErrUnexpectedEOF
					}
					v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
					iNdEx += 8
					v2 := float64(math.Float64frombits(v))
					m.Inputs = append(m.Inputs, v2)
				}
			} else {
				return fmt.Errorf("proto: wrong wireType = %d for field Inputs", wireType)
			}
		case 2:
			if wireType == 1 {
				var v uint64
				if (iNdEx + 8) > l {
					return io.ErrUnexpectedEOF
				}
				v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
				iNdEx += 8
				v2 := float64(math.Float64frombits(v))
				m.Outputs = append(m.Outputs, v2)
			} else if wireType == 2 {
				var packedLen int
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowNn
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					packedLen |= (int(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				if packedLen < 0 {
					return ErrInvalidLengthNn
				}
				postIndex := iNdEx + packedLen
				if postIndex > l {
					return io.ErrUnexpectedEOF
				}
				for iNdEx < postIndex {
					var v uint64
					if (iNdEx + 8) > l {
						return io.ErrUnexpectedEOF
					}
					v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
					iNdEx += 8
					v2 := float64(math.Float64frombits(v))
					m.Outputs = append(m.Outputs, v2)
				}
			} else {
				return fmt.Errorf("proto: wrong wireType = %d for field Outputs", wireType)
			}
		default:
			iNdEx = preIndex
			skippy, err := skipNn(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthNn
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *TrainingData) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowNn
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: TrainingData: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: TrainingData: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Examples", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowNn
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthNn
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Examples = append(m.Examples, &Data{})
			if err := m.Examples[len(m.Examples)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipNn(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthNn
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *LayerConfig) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowNn
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: LayerConfig: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: LayerConfig: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field NodeCount", wireType)
			}
			m.NodeCount = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowNn
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.NodeCount |= (uint32(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Activation", wireType)
			}
			m.Activation = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowNn
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Activation |= (Activation(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		default:
			iNdEx = preIndex
			skippy, err := skipNn(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthNn
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *LayerData) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowNn
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: LayerData: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: LayerData: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field NodeCount", wireType)
			}
			m.NodeCount = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowNn
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.NodeCount |= (uint32(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType == 1 {
				var v uint64
				if (iNdEx + 8) > l {
					return io.ErrUnexpectedEOF
				}
				v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
				iNdEx += 8
				v2 := float64(math.Float64frombits(v))
				m.WeightsAndBiases = append(m.WeightsAndBiases, v2)
			} else if wireType == 2 {
				var packedLen int
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowNn
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					packedLen |= (int(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				if packedLen < 0 {
					return ErrInvalidLengthNn
				}
				postIndex := iNdEx + packedLen
				if postIndex > l {
					return io.ErrUnexpectedEOF
				}
				for iNdEx < postIndex {
					var v uint64
					if (iNdEx + 8) > l {
						return io.ErrUnexpectedEOF
					}
					v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
					iNdEx += 8
					v2 := float64(math.Float64frombits(v))
					m.WeightsAndBiases = append(m.WeightsAndBiases, v2)
				}
			} else {
				return fmt.Errorf("proto: wrong wireType = %d for field WeightsAndBiases", wireType)
			}
		case 3:
			if wireType == 1 {
				var v uint64
				if (iNdEx + 8) > l {
					return io.ErrUnexpectedEOF
				}
				v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
				iNdEx += 8
				v2 := float64(math.Float64frombits(v))
				m.Velocities = append(m.Velocities, v2)
			} else if wireType == 2 {
				var packedLen int
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowNn
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					packedLen |= (int(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				if packedLen < 0 {
					return ErrInvalidLengthNn
				}
				postIndex := iNdEx + packedLen
				if postIndex > l {
					return io.ErrUnexpectedEOF
				}
				for iNdEx < postIndex {
					var v uint64
					if (iNdEx + 8) > l {
						return io.ErrUnexpectedEOF
					}
					v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
					iNdEx += 8
					v2 := float64(math.Float64frombits(v))
					m.Velocities = append(m.Velocities, v2)
				}
			} else {
				return fmt.Errorf("proto: wrong wireType = %d for field Velocities", wireType)
			}
		case 4:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Activation", wireType)
			}
			m.Activation = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowNn
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Activation |= (Activation(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		default:
			iNdEx = preIndex
			skippy, err := skipNn(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthNn
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *Position) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowNn
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: Position: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: Position: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType == 1 {
				var v uint64
				if (iNdEx + 8) > l {
					return io.ErrUnexpectedEOF
				}
				v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
				iNdEx += 8
				v2 := float64(math.Float64frombits(v))
				m.WeightsAndBiases = append(m.WeightsAndBiases, v2)
			} else if wireType == 2 {
				var packedLen int
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowNn
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					packedLen |= (int(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				if packedLen < 0 {
					return ErrInvalidLengthNn
				}
				postIndex := iNdEx + packedLen
				if postIndex > l {
					return io.ErrUnexpectedEOF
				}
				for iNdEx < postIndex {
					var v uint64
					if (iNdEx + 8) > l {
						return io.ErrUnexpectedEOF
					}
					v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
					iNdEx += 8
					v2 := float64(math.Float64frombits(v))
					m.WeightsAndBiases = append(m.WeightsAndBiases, v2)
				}
			} else {
				return fmt.Errorf("proto: wrong wireType = %d for field WeightsAndBiases", wireType)
			}
		case 2:
			if wireType != 1 {
				return fmt.Errorf("proto: wrong wireType = %d for field Loss", wireType)
			}
			var v uint64
			if (iNdEx + 8) > l {
				return io.ErrUnexpectedEOF
			}
			v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
			iNdEx += 8
			m.Loss = float64(math.Float64frombits(v))
		default:
			iNdEx = preIndex
			skippy, err := skipNn(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthNn
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *NeuralNetworkConfiguration) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowNn
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: NeuralNetworkConfiguration: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: NeuralNetworkConfiguration: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Loss", wireType)
			}
			m.Loss = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowNn
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Loss |= (Loss(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field InputCount", wireType)
			}
			m.InputCount = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowNn
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.InputCount |= (uint32(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field LayerConfigs", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowNn
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthNn
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.LayerConfigs = append(m.LayerConfigs, &LayerConfig{})
			if err := m.LayerConfigs[len(m.LayerConfigs)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipNn(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthNn
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *NeuralNetworkData) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowNn
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: NeuralNetworkData: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: NeuralNetworkData: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Loss", wireType)
			}
			m.Loss = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowNn
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Loss |= (Loss(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Layers", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowNn
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthNn
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Layers = append(m.Layers, &LayerData{})
			if err := m.Layers[len(m.Layers)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 3:
			if wireType != 1 {
				return fmt.Errorf("proto: wrong wireType = %d for field CurrentLoss", wireType)
			}
			var v uint64
			if (iNdEx + 8) > l {
				return io.ErrUnexpectedEOF
			}
			v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
			iNdEx += 8
			m.CurrentLoss = float64(math.Float64frombits(v))
		case 4:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Best", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowNn
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthNn
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.Best == nil {
				m.Best = &Position{}
			}
			if err := m.Best.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipNn(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthNn
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *TrainingConfiguration) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowNn
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: TrainingConfiguration: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: TrainingConfiguration: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 1 {
				return fmt.Errorf("proto: wrong wireType = %d for field InertiaWeight", wireType)
			}
			var v uint64
			if (iNdEx + 8) > l {
				return io.ErrUnexpectedEOF
			}
			v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
			iNdEx += 8
			m.InertiaWeight = float64(math.Float64frombits(v))
		case 2:
			if wireType != 1 {
				return fmt.Errorf("proto: wrong wireType = %d for field CognitiveWeight", wireType)
			}
			var v uint64
			if (iNdEx + 8) > l {
				return io.ErrUnexpectedEOF
			}
			v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
			iNdEx += 8
			m.CognitiveWeight = float64(math.Float64frombits(v))
		case 3:
			if wireType != 1 {
				return fmt.Errorf("proto: wrong wireType = %d for field SocialWeight", wireType)
			}
			var v uint64
			if (iNdEx + 8) > l {
				return io.ErrUnexpectedEOF
			}
			v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
			iNdEx += 8
			m.SocialWeight = float64(math.Float64frombits(v))
		case 4:
			if wireType != 1 {
				return fmt.Errorf("proto: wrong wireType = %d for field GlobalWeight", wireType)
			}
			var v uint64
			if (iNdEx + 8) > l {
				return io.ErrUnexpectedEOF
			}
			v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
			iNdEx += 8
			m.GlobalWeight = float64(math.Float64frombits(v))
		case 5:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field MaxIterations", wireType)
			}
			m.MaxIterations = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowNn
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.MaxIterations |= (uint32(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 6:
			if wireType != 1 {
				return fmt.Errorf("proto: wrong wireType = %d for field TargetAccuracy", wireType)
			}
			var v uint64
			if (iNdEx + 8) > l {
				return io.ErrUnexpectedEOF
			}
			v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
			iNdEx += 8
			m.TargetAccuracy = float64(math.Float64frombits(v))
		case 7:
			if wireType != 1 {
				return fmt.Errorf("proto: wrong wireType = %d for field WeightRange", wireType)
			}
			var v uint64
			if (iNdEx + 8) > l {
				return io.ErrUnexpectedEOF
			}
			v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
			iNdEx += 8
			m.WeightRange = float64(math.Float64frombits(v))
		case 8:
			if wireType != 1 {
				return fmt.Errorf("proto: wrong wireType = %d for field WeightDecayRate", wireType)
			}
			var v uint64
			if (iNdEx + 8) > l {
				return io.ErrUnexpectedEOF
			}
			v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
			iNdEx += 8
			m.WeightDecayRate = float64(math.Float64frombits(v))
		case 9:
			if wireType != 1 {
				return fmt.Errorf("proto: wrong wireType = %d for field ProbablityOfDeath", wireType)
			}
			var v uint64
			if (iNdEx + 8) > l {
				return io.ErrUnexpectedEOF
			}
			v = uint64(binary.LittleEndian.Uint64(dAtA[iNdEx:]))
			iNdEx += 8
			m.ProbablityOfDeath = float64(math.Float64frombits(v))
		default:
			iNdEx = preIndex
			skippy, err := skipNn(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthNn
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *MultiSwarmConfiguration) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowNn
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: MultiSwarmConfiguration: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: MultiSwarmConfiguration: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field NeuralNetworkConfiguration", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowNn
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthNn
			}
			postIndex := iNdEx + msglen
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.NeuralNetworkConfiguration == nil {
				m.NeuralNetworkConfiguration = &NeuralNetworkConfiguration{}
			}
			if err := m.NeuralNetworkConfiguration.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field SwarmCount", wireType)
			}
			m.SwarmCount = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowNn
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.SwarmCount |= (uint32(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ParticleCount", wireType)
			}
			m.ParticleCount = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowNn
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ParticleCount |= (uint32(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		default:
			iNdEx = preIndex
			skippy, err := skipNn(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthNn
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func skipNn(dAtA []byte) (n int, err error) {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return 0, ErrIntOverflowNn
			}
			if iNdEx >= l {
				return 0, io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		wireType := int(wire & 0x7)
		switch wireType {
		case 0:
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowNn
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				iNdEx++
				if dAtA[iNdEx-1] < 0x80 {
					break
				}
			}
			return iNdEx, nil
		case 1:
			iNdEx += 8
			return iNdEx, nil
		case 2:
			var length int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowNn
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				length |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			iNdEx += length
			if length < 0 {
				return 0, ErrInvalidLengthNn
			}
			return iNdEx, nil
		case 3:
			for {
				var innerWire uint64
				var start int = iNdEx
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return 0, ErrIntOverflowNn
					}
					if iNdEx >= l {
						return 0, io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					innerWire |= (uint64(b) & 0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				innerWireType := int(innerWire & 0x7)
				if innerWireType == 4 {
					break
				}
				next, err := skipNn(dAtA[start:])
				if err != nil {
					return 0, err
				}
				iNdEx = start + next
			}
			return iNdEx, nil
		case 4:
			return iNdEx, nil
		case 5:
			iNdEx += 4
			return iNdEx, nil
		default:
			return 0, fmt.Errorf("proto: illegal wireType %d", wireType)
		}
	}
	panic("unreachable")
}

var (
	ErrInvalidLengthNn = fmt.Errorf("proto: negative length found during unmarshaling")
	ErrIntOverflowNn   = fmt.Errorf("proto: integer overflow")
)

func init() { proto.RegisterFile("nn.proto", fileDescriptorNn) }

var fileDescriptorNn = []byte{
	// 988 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x8c, 0x55, 0xcf, 0x73, 0x1b, 0x35,
	0x14, 0xb6, 0x62, 0xc7, 0xb1, 0xdf, 0xda, 0x8e, 0xac, 0x52, 0x58, 0x3a, 0x74, 0x1b, 0x4c, 0x3b,
	0xa4, 0x9d, 0x4e, 0x0e, 0xe6, 0x92, 0x6b, 0x7e, 0x74, 0x4a, 0x06, 0xb7, 0x74, 0xd6, 0xcd, 0x70,
	0x5c, 0xe4, 0xb5, 0xb2, 0xd5, 0x64, 0x23, 0x19, 0x49, 0x9b, 0xc4, 0x9c, 0xe0, 0x3f, 0xe0, 0xca,
	0x7f, 0x40, 0x67, 0xf8, 0x3f, 0xe0, 0xd8, 0x23, 0x47, 0x62, 0x2e, 0x1c, 0xfb, 0x0f, 0x30, 0xc3,
	0x48, 0xda, 0xb5, 0x1b, 0x66, 0x32, 0xe5, 0x26, 0x7d, 0xdf, 0xe7, 0xf7, 0xbe, 0xfd, 0x9e, 0x5e,
	0x02, 0x2d, 0x21, 0x76, 0x66, 0x4a, 0x1a, 0x49, 0x9a, 0xa9, 0xcc, 0x98, 0x30, 0x83, 0x5d, 0x68,
	0x1c, 0x52, 0x43, 0xc9, 0x87, 0xd0, 0xe4, 0x62, 0x56, 0x18, 0x1d, 0xa2, 0xad, 0xfa, 0x36, 0x8a,
	0xcb, 0x1b, 0x09, 0x61, 0x43, 0x16, 0xc6, 0x11, 0x6b, 0x8e, 0xa8, 0xae, 0x83, 0x5d, 0xe8, 0xbc,
	0x54, 0x94, 0x0b, 0x2e, 0x32, 0x57, 0x61, 0x1b, 0x5a, 0xec, 0x92, 0x9e, 0xcd, 0x72, 0xe6, 0x6b,
	0x04, 0xc3, 0xce, 0x8e, 0x6f, 0xb2, 0x63, 0xf9, 0x78, 0xc9, 0x0e, 0xbe, 0x85, 0x60, 0x44, 0xe7,
	0x4c, 0x1d, 0x48, 0x71, 0xc2, 0x33, 0x72, 0x17, 0x40, 0xc8, 0x29, 0x4b, 0x52, 0x59, 0x08, 0x13,
	0xa2, 0x2d, 0xb4, 0xdd, 0x8d, 0xdb, 0x16, 0x39, 0xb0, 0x00, 0x19, 0x02, 0xd0, 0xd4, 0xf0, 0x73,
	0x6a, 0xb8, 0x14, 0xe1, 0xda, 0x16, 0xda, 0xee, 0x0d, 0x49, 0x55, 0x79, 0x6f, 0xc9, 0xc4, 0xef,
	0xa8, 0x06, 0xbf, 0x22, 0x68, 0xbb, 0x16, 0xce, 0xd9, 0x7b, 0x1a, 0x3c, 0x06, 0x72, 0xc1, 0x78,
	0xf6, 0xca, 0xe8, 0x84, 0x8a, 0x69, 0x32, 0xe1, 0x54, 0xb3, 0xea, 0x6b, 0x71, 0xc9, 0xec, 0x89,
	0xe9, 0xbe, 0xc3, 0x49, 0x04, 0x70, 0xce, 0x72, 0x99, 0x72, 0xc3, 0x99, 0x0e, 0xeb, 0x4e, 0xf5,
	0x0e, 0xf2, 0x1f, 0xbb, 0x8d, 0xff, 0x65, 0x77, 0x04, 0xad, 0x17, 0x52, 0x73, 0x7b, 0xbe, 0xc1,
	0x0d, 0xba, 0xc1, 0x0d, 0x81, 0x46, 0x2e, 0xb5, 0x76, 0xb1, 0xa0, 0xd8, 0x9d, 0x07, 0x3f, 0x23,
	0xb8, 0xf3, 0x9c, 0x15, 0x8a, 0xe6, 0xcf, 0x99, 0xb9, 0x90, 0xea, 0xd4, 0xe7, 0x5c, 0x28, 0xd7,
	0x8c, 0x6c, 0x95, 0x3f, 0x41, 0xce, 0xda, 0x72, 0x46, 0x23, 0xa9, 0xb5, 0x2f, 0x40, 0xee, 0x41,
	0xe0, 0xa6, 0x5f, 0x06, 0xb6, 0xe6, 0x02, 0x03, 0x07, 0xf9, 0xc4, 0x76, 0xa1, 0x9b, 0xdb, 0x74,
	0x93, 0xd4, 0x55, 0xf6, 0x31, 0x04, 0xc3, 0x5b, 0xcb, 0x5a, 0xab, 0xe9, 0xc6, 0x9d, 0x7c, 0x75,
	0xd1, 0x83, 0xd7, 0x08, 0xfa, 0xd7, 0xbc, 0xb9, 0x01, 0xbd, 0xdf, 0xd2, 0x43, 0x68, 0xba, 0x3a,
	0x7e, 0x2e, 0xc1, 0xb0, 0x7f, 0xad, 0x95, 0x7b, 0x5f, 0xa5, 0x80, 0x7c, 0x0a, 0x9d, 0xb4, 0x50,
	0x8a, 0x09, 0x93, 0xb8, 0xa2, 0x75, 0x17, 0x4d, 0x50, 0x62, 0xb6, 0x26, 0xb9, 0x0f, 0x8d, 0x09,
	0xd3, 0xc6, 0x4d, 0x27, 0x18, 0xe2, 0xaa, 0x56, 0x35, 0x83, 0xd8, 0xb1, 0x83, 0x1f, 0xeb, 0x70,
	0xbb, 0x7a, 0xe1, 0xd7, 0x23, 0x7c, 0x00, 0x3d, 0x2e, 0x98, 0x32, 0x9c, 0x26, 0x7e, 0x22, 0xce,
	0x39, 0x8a, 0xbb, 0x25, 0xfa, 0x8d, 0x03, 0xc9, 0x43, 0xc0, 0xa9, 0xcc, 0x04, 0x37, 0xfc, 0x9c,
	0x55, 0x42, 0x3f, 0xa8, 0xcd, 0x25, 0x5e, 0x4a, 0x3f, 0x83, 0xae, 0x96, 0x29, 0xa7, 0x79, 0xa5,
	0xf3, 0xae, 0x3b, 0x1e, 0x5c, 0x89, 0xb2, 0x5c, 0x4e, 0x56, 0xa2, 0x86, 0x17, 0x79, 0xb0, 0x14,
	0xdd, 0x87, 0xee, 0x19, 0xbd, 0x3c, 0x32, 0xcc, 0x7b, 0xd5, 0xe1, 0xba, 0x1b, 0xdf, 0x75, 0x90,
	0x7c, 0x0e, 0x9b, 0x86, 0xaa, 0x8c, 0x99, 0x84, 0xa6, 0x69, 0xa1, 0x68, 0x3a, 0x0f, 0x9b, 0xae,
	0x58, 0xcf, 0xc3, 0x7b, 0x25, 0x6a, 0xd3, 0xf4, 0xcd, 0x12, 0x45, 0x45, 0xc6, 0xc2, 0x0d, 0x9f,
	0xa6, 0xc7, 0x62, 0x0b, 0x91, 0x47, 0xd0, 0x2f, 0x25, 0x53, 0x96, 0xd2, 0x79, 0xa2, 0xa8, 0x61,
	0x61, 0xcb, 0x7f, 0xa7, 0x27, 0x0e, 0x2d, 0x1e, 0x53, 0xc3, 0xc8, 0x0e, 0xdc, 0x9a, 0x29, 0x39,
	0xa1, 0x93, 0x9c, 0x9b, 0x79, 0x22, 0x4f, 0x92, 0x29, 0xa3, 0xe6, 0x55, 0xd8, 0x76, 0xea, 0xfe,
	0x8a, 0xfa, 0xfa, 0xe4, 0xd0, 0x12, 0x83, 0xdf, 0x10, 0x7c, 0xf4, 0xac, 0xc8, 0x0d, 0x1f, 0x5f,
	0x50, 0x75, 0x76, 0x7d, 0x0a, 0x53, 0xf8, 0x44, 0xb8, 0xa7, 0x94, 0x08, 0xff, 0x96, 0xca, 0xe7,
	0x58, 0xf2, 0x2e, 0xea, 0x60, 0x38, 0xa8, 0xa6, 0x7b, 0xf3, 0x4a, 0xc4, 0x77, 0xc4, 0xcd, 0xeb,
	0x72, 0x0f, 0x02, 0x6d, 0x7b, 0x97, 0xcb, 0x50, 0xf7, 0xcb, 0xa0, 0xbd, 0x1d, 0xbb, 0x0c, 0x0f,
	0xa0, 0x37, 0xa3, 0xca, 0xf0, 0x34, 0xaf, 0xfe, 0xc2, 0x34, 0x7c, 0xe2, 0x15, 0xea, 0x64, 0x8f,
	0xfe, 0x41, 0x00, 0xab, 0xf5, 0x27, 0x1d, 0x68, 0x1d, 0x4d, 0x99, 0x30, 0xdc, 0xcc, 0x71, 0x8d,
	0xf4, 0x00, 0xf6, 0xb9, 0xa0, 0x6a, 0x3e, 0x36, 0x6c, 0x86, 0x11, 0x09, 0x60, 0x63, 0xcc, 0xb3,
	0x33, 0xc9, 0xa7, 0x78, 0x8d, 0xdc, 0x86, 0xfe, 0x97, 0xf3, 0x19, 0x53, 0x13, 0x99, 0xf3, 0xf4,
	0xa5, 0x8d, 0x5c, 0x18, 0x5c, 0x27, 0x00, 0xcd, 0x3d, 0x65, 0xef, 0xb8, 0x61, 0xab, 0x8d, 0xe5,
	0x89, 0xd1, 0x3c, 0x13, 0x78, 0x9d, 0xb4, 0xa0, 0x71, 0x34, 0x8e, 0x8f, 0x71, 0xd3, 0x9e, 0x62,
	0x36, 0x3a, 0xc6, 0x1b, 0xa4, 0x0b, 0xed, 0x11, 0xa3, 0xa7, 0x73, 0x77, 0x6d, 0x91, 0x0d, 0xa8,
	0x3f, 0x19, 0x1d, 0xe3, 0xb6, 0x55, 0x8c, 0xed, 0x09, 0xaa, 0x1a, 0x2f, 0xf2, 0x42, 0xe3, 0x80,
	0x60, 0xe8, 0xec, 0x33, 0x61, 0x96, 0x1e, 0x3b, 0x8e, 0xe7, 0xa2, 0xd0, 0xd6, 0x54, 0xd7, 0xfd,
	0x8e, 0x8b, 0x14, 0xf7, 0x2c, 0xfe, 0x94, 0x16, 0x5a, 0x73, 0x2a, 0xf0, 0xa6, 0x73, 0x2e, 0x4f,
	0xcc, 0x19, 0xbd, 0xc4, 0xd8, 0x5a, 0x7c, 0x46, 0x2f, 0x65, 0x61, 0x70, 0xff, 0xd1, 0x6b, 0x04,
	0x0d, 0xb7, 0x7c, 0x56, 0xf1, 0x5d, 0x41, 0x15, 0x9b, 0xe2, 0x1a, 0x69, 0xc3, 0xfa, 0x81, 0x92,
	0x5a, 0x63, 0x44, 0x36, 0x21, 0x78, 0x72, 0x39, 0x93, 0xc2, 0x36, 0xa4, 0x79, 0xf9, 0xdd, 0x2c,
	0xcf, 0xb9, 0xc8, 0x98, 0x3a, 0xe4, 0xda, 0x50, 0x91, 0x32, 0x5c, 0x27, 0x77, 0xe1, 0xe3, 0xaf,
	0x8a, 0x3c, 0x9f, 0xd0, 0xf4, 0x74, 0xc4, 0xf8, 0x24, 0xb7, 0xe4, 0x39, 0x53, 0x19, 0xb3, 0x74,
	0x83, 0x6c, 0xc3, 0xfd, 0xa7, 0x4c, 0x30, 0x45, 0x73, 0xfe, 0x3d, 0x9b, 0xde, 0xac, 0x5c, 0x27,
	0x21, 0x7c, 0x70, 0x64, 0xe8, 0x69, 0xa1, 0xe8, 0x98, 0x72, 0x23, 0x97, 0x2d, 0x9a, 0xfb, 0x8f,
	0xdf, 0x5c, 0x45, 0xb5, 0x3f, 0xae, 0xa2, 0xda, 0xdb, 0xab, 0x08, 0xfd, 0xb0, 0x88, 0xd0, 0x2f,
	0x8b, 0x08, 0xfd, 0xbe, 0x88, 0xd0, 0x9b, 0x45, 0x84, 0xfe, 0x5c, 0x44, 0xe8, 0xef, 0x45, 0x54,
	0x7b, 0xbb, 0x88, 0xd0, 0x4f, 0x7f, 0x45, 0xb5, 0x49, 0xd3, 0xfd, 0x47, 0xfd, 0xe2, 0xdf, 0x00,
	0x00, 0x00, 0xff, 0xff, 0x35, 0xc4, 0x76, 0x00, 0x5d, 0x07, 0x00, 0x00,
}
